{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing the website (serial number 8 ): https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Exception: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error accessing the website (serial number 21 ): https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Exception: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Error accessing the website (serial number 108 ): https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n",
      "Exception: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
     ]
    }
   ],
   "source": [
    "#only type1 websites\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the XLSX file\n",
    "xlsx_file = 'Input.xlsx'\n",
    "df = pd.read_excel(xlsx_file)\n",
    "\n",
    "# Extract the URLs from the Excel file\n",
    "urls = df['URL'].tolist()\n",
    "\n",
    "x=0\n",
    "# Iterate over the URLs\n",
    "for url in urls:\n",
    "    x=x+1\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        divs=soup.find_all('div', class_='tdb-block-inner td-fix-index')\n",
    "        article=[]\n",
    "    \n",
    "        for div in divs:\n",
    "            temp=[]\n",
    "            title=div.find('h1', class_='tdb-title-text')\n",
    "            if title:\n",
    "                article.append(title.text)\n",
    "            paragraphs=div.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                para=paragraph.text\n",
    "                if para:\n",
    "                    temp.append(para)\n",
    "            if len(temp)>0:\n",
    "                article.append(temp)\n",
    "            temp=[]\n",
    "            \n",
    "            \n",
    "        #write the article to a text file\n",
    "        if article:\n",
    "            filename='ExtractedArticles/'+str(df['URL_ID'][x-1])+'.txt'\n",
    "            with open(filename, 'w') as file:\n",
    "                for i in article:\n",
    "                    try:\n",
    "                        file.write(i)\n",
    "                        file.write('\\n')\n",
    "                    except TypeError as e:\n",
    "                        for j in i:\n",
    "                            file.write(j)\n",
    "                            file.write('\\n')\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Error accessing the website (serial number',x,'):', url)\n",
    "        print('Exception:', e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error accessing the website (serial number 8 ): https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Exception: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/how-neural-networks-can-be-applied-in-various-areas-in-the-future/\n",
      "Error accessing the website (serial number 21 ): https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Exception: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/covid-19-environmental-impact-for-the-future/\n",
      "Error accessing the website (serial number 108 ): https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n",
      "Exception: 404 Client Error: Not Found for url: https://insights.blackcoffer.com/ensuring-growth-through-insurance-technology/\n"
     ]
    }
   ],
   "source": [
    "#only type2 websites\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the XLSX file\n",
    "xlsx_file = 'Input.xlsx'\n",
    "df = pd.read_excel(xlsx_file)\n",
    "\n",
    "# Extract the URLs from the Excel file\n",
    "urls = df['URL'].tolist()\n",
    "\n",
    "x=0\n",
    "# Iterate over the URLs\n",
    "for url in urls:\n",
    "    x=x+1\n",
    "    try:\n",
    "        # Send a GET request to the website\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Parse the HTML content\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        divs=soup.find_all('div', class_='td-parallax-header')\n",
    "        article=[]\n",
    "    \n",
    "        for div in divs:\n",
    "            title=div.find('h1', class_='entry-title')\n",
    "            if title:\n",
    "                article.append(title.text)\n",
    "        divs=soup.find_all('div', class_='td-post-content tagdiv-type')\n",
    "        for div in divs:\n",
    "            temp=[]\n",
    "            paragraphs=div.find_all('p')\n",
    "            for paragraph in paragraphs:\n",
    "                para=paragraph.text\n",
    "                if para:\n",
    "                    temp.append(para)\n",
    "            if len(temp)>0:\n",
    "                article.append(temp)\n",
    "            temp=[]\n",
    "\n",
    "\n",
    "        #write the article to a text file\n",
    "        if article:\n",
    "            filename='ExtractedArticles/'+str(df['URL_ID'][x-1])+'.txt'\n",
    "            with open(filename, 'w') as file:\n",
    "                for i in article:\n",
    "                    try:\n",
    "                        file.write(i)\n",
    "                        file.write('\\n')\n",
    "                    except TypeError as e:\n",
    "                        for j in i:\n",
    "                            file.write(j)\n",
    "                            file.write('\\n')\n",
    "                            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print('Error accessing the website (serial number',x,'):', url)\n",
    "        print('Exception:', e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\benso\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File not found: ExtractedArticles/44.txt\n",
      "Exception: [Errno 2] No such file or directory: 'ExtractedArticles/44.txt'\n",
      "File not found: ExtractedArticles/57.txt\n",
      "Exception: [Errno 2] No such file or directory: 'ExtractedArticles/57.txt'\n",
      "File not found: ExtractedArticles/144.txt\n",
      "Exception: [Errno 2] No such file or directory: 'ExtractedArticles/144.txt'\n"
     ]
    }
   ],
   "source": [
    "#Text Analysis\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    " \n",
    "import re\n",
    "def count_syllables(word):\n",
    "    word = word.lower()\n",
    "    if len(word) <= 3:\n",
    "        return 1\n",
    "\n",
    "    # Remove non-alphabetic characters\n",
    "    word = re.sub(r'[^a-z]', '', word)\n",
    "\n",
    "    # Handle exceptions like words ending with \"es\" or \"ed\"\n",
    "    exceptions = ['es', 'ed']\n",
    "    for exception in exceptions:\n",
    "        if word.endswith(exception):\n",
    "            word = word[:-len(exception)]\n",
    "            break\n",
    "\n",
    "    # Count syllables based on vowel patterns\n",
    "    vowels = re.findall(r'[aeiouy]+', word)\n",
    "    return max(1, len(vowels))\n",
    "\n",
    "def is_personal_pronouns(word):\n",
    "    pronounRegex = re.compile(r'\\b(I|we|my|ours|(?-i:us))\\b',re.I)\n",
    "    if pronounRegex.search(word):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def is_complex_word(word):\n",
    "    syllable_count = count_syllables(word)\n",
    "    return syllable_count > 2\n",
    "\n",
    "import os\n",
    "\n",
    "directory = 'StopWords'\n",
    "stopwords = []\n",
    "positive_words=[]\n",
    "negative_words=[]\n",
    "\n",
    "encodings = ['utf-8', 'latin-1', 'utf-16']  # Add more encodings if needed\n",
    "\n",
    "# Iterate over files in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    file_path = os.path.join(directory, filename)\n",
    "    if filename.endswith('.txt') and os.path.isfile(file_path):\n",
    "        # Try different encodings to read the file\n",
    "        for encoding in encodings:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    file_content = file.read().split()\n",
    "                    for word in file_content:\n",
    "                        stopwords.append(word)\n",
    "                # Append the file content to the list\n",
    "                \n",
    "                break  # Break the loop if a successful encoding is found\n",
    "\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try the next encoding if decoding fails\n",
    "\n",
    "with open('MasterDictionary/negative-words.txt', 'r', encoding='latin-1') as file:\n",
    "            file_content = file.read().split()\n",
    "            for word in file_content:\n",
    "                if word not in stopwords:\n",
    "                    negative_words.append(word)\n",
    "\n",
    "for encoding in encodings:\n",
    "    try:\n",
    "        with open('MasterDictionary/positive-words.txt', 'r') as file:\n",
    "            file_content = file.read().split()\n",
    "            for word in file_content:\n",
    "                if word not in stopwords:\n",
    "                    positive_words.append(word)\n",
    "        break\n",
    "    except UnicodeDecodeError:\n",
    "        continue\n",
    "\n",
    "output_file='output.xlsx'\n",
    "wb_obj = openpyxl.load_workbook(output_file)\n",
    "sheet_obj = wb_obj.active\n",
    "row_index=1\n",
    "\n",
    "\n",
    "punc = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "df = pd.read_excel(output_file)\n",
    "url_id = df['URL_ID'].tolist()\n",
    "for id in url_id:\n",
    "    row_index=row_index+1\n",
    "    filename='ExtractedArticles/'+str(id)+'.txt'\n",
    "    try:\n",
    "        with open(filename, 'r') as file:\n",
    "            positive_score=0\n",
    "            negative_score=0\n",
    "            polarity_score=0\n",
    "            subjectivity_score=0\n",
    "            sentence_count=0\n",
    "            avg_sentence_length=0\n",
    "            complex_word_count=0\n",
    "            percentage_complex_words=0\n",
    "            fog_index=0\n",
    "            avg_words_per_sentence=0\n",
    "            syllable_count_per_word=0\n",
    "            avg_word_length=0\n",
    "            personal_pronouns=0\n",
    "\n",
    "            data=file.read()\n",
    "            tokens=word_tokenize(data)\n",
    "            for word in tokens:\n",
    "                if word in stopwords:\n",
    "                    tokens.remove(word)\n",
    "                if word == '.':\n",
    "                    sentence_count=sentence_count+1\n",
    "                if word in punc:\n",
    "                    tokens.remove(word)\n",
    "            for word in tokens:\n",
    "                if is_personal_pronouns(word):\n",
    "                    personal_pronouns=personal_pronouns+1\n",
    "                avg_word_length=avg_word_length+len(word)\n",
    "                syllable_count_per_word=syllable_count_per_word+count_syllables(word)\n",
    "                if is_complex_word(word):\n",
    "                    complex_word_count=complex_word_count+1\n",
    "                if word in positive_words:\n",
    "                    positive_score=positive_score+1\n",
    "                elif word in negative_words:\n",
    "                    negative_score=negative_score-1\n",
    "                    negative_score=negative_score*(-1)\n",
    "            polarity_score=(positive_score - negative_score)/((positive_score + negative_score)+0.000001)\n",
    "            subjectivity_score=(positive_score + negative_score)/(len(tokens)+0.000001)\n",
    "            avg_sentence_length=len(tokens)/sentence_count\n",
    "            percentage_complex_words=complex_word_count/len(tokens)\n",
    "            fog_index=0.4*(avg_sentence_length+percentage_complex_words)\n",
    "            avg_words_per_sentence=avg_sentence_length\n",
    "            avg_word_length=avg_word_length/len(tokens)\n",
    "            sheet_obj.cell(row=row_index, column=3).value = positive_score\n",
    "            sheet_obj.cell(row=row_index, column=4).value = negative_score\n",
    "            sheet_obj.cell(row=row_index, column=5).value = polarity_score\n",
    "            sheet_obj.cell(row=row_index, column=6).value = subjectivity_score\n",
    "            sheet_obj.cell(row=row_index, column=7).value = avg_sentence_length\n",
    "            sheet_obj.cell(row=row_index, column=8).value = percentage_complex_words\n",
    "            sheet_obj.cell(row=row_index, column=9).value = fog_index\n",
    "            sheet_obj.cell(row=row_index, column=10).value = avg_words_per_sentence\n",
    "            sheet_obj.cell(row=row_index, column=11).value = complex_word_count\n",
    "            sheet_obj.cell(row=row_index, column=12).value = len(tokens)\n",
    "            sheet_obj.cell(row=row_index, column=13).value = syllable_count_per_word\n",
    "            sheet_obj.cell(row=row_index, column=14).value = personal_pronouns\n",
    "            sheet_obj.cell(row=row_index, column=15).value = avg_word_length\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        print('File not found:', filename)\n",
    "        print('Exception:', e)\n",
    "    wb_obj.save(output_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
